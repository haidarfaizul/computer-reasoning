{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyjZv_08r3Cd",
        "outputId": "773884fe-f41f-458d-9089-1bbf44293c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieval Metrics Table:\n",
            "/content/drive/MyDrive/Tugasbesar/data/eval/retrieval_metrics.csv\n",
            "\n",
            "     Model   Accuracy   Precision  Recall    F1\n",
            "0  TF-IDF       0.80       0.80      0.80    0.80\n",
            "1    BERT       0.80       0.66      0.66    0.66\n",
            "\n",
            "Prediction metrics saved to \n",
            "/content/drive/MyDrive/Tugasbesar/data/eval/prediction_metrics.csv\n",
            "Average Prediction Accuracy: 0.0000\n",
            "\n",
            "Error Analysis - Retrieval Failures:\n",
            "Query: Sengketa perdata mengenai hutang piutang dengan penyelesaian damai\n",
            "Ground Truth ID: 1\n",
            "TF-IDF Retrieved: [118, 123, 93, 53, 11] (Similarities: [0.10895861031516804, 0.08362106563953889, 0.07773682547362205, 0.07576645602701834, 0.07269386733096407])\n",
            "BERT Retrieved: [117, 57, 67, 78, 13] (Similarities: [0.4797317385673523, 0.4768753945827484, 0.47631460428237915, 0.46995988488197327, 0.4691508412361145])\n",
            "\n",
            "Query: Wanprestasi pembayaran atas perjanjian investasi\n",
            "Ground Truth ID: 2\n",
            "TF-IDF Retrieved: [68, 24, 80, 42, 125] (Similarities: [0.25851384150252066, 0.1678201642505457, 0.14196763180908048, 0.11600392780471092, 0.10662562375128859])\n",
            "BERT Retrieved: [117, 113, 29, 73, 85] (Similarities: [0.43551844358444214, 0.41558951139450073, 0.41143494844436646, 0.4068748354911804, 0.4052280783653259])\n",
            "\n",
            "Query: Gugatan perdata dengan bukti transfer dan pengakuan hutang\n",
            "Ground Truth ID: 3\n",
            "TF-IDF Retrieved: [118, 11, 53, 121, 126] (Similarities: [0.2504534354698041, 0.1863112700555842, 0.16838035490188713, 0.16171440394793335, 0.15503043089948912])\n",
            "BERT Retrieved: [67, 57, 117, 61, 4] (Similarities: [0.4968058466911316, 0.49075746536254883, 0.48312580585479736, 0.4827861189842224, 0.4820397198200226])\n",
            "...\n",
            "Predicted Solution: sah dan berharga sita jaminan atas 1 (satu) bidangtanah dan/atau tanah dan bangunan:di jl\n",
            "Ground Truth Solution: Tergugat dinyatakan wanprestasi dan wajib membayar hutang\n",
            "Top-5 Case IDs: [57, 118, 46, 54, 29]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import json\n",
        "from typing import List, Dict\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(file_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['combined_text'] = df['case_summary'].fillna('') + ' ' + df['full_text'].fillna('')\n",
        "    def clean_text(text: str) -> str:\n",
        "        # Preserve legal terms by limiting punctuation removal\n",
        "        text = re.sub(r'[\\n\\t]', ' ', text.lower())  # Remove newlines and tabs only\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    df['combined_text'] = df['combined_text'].apply(clean_text)\n",
        "    df['solution'] = df['legal_basis'].fillna(df['case_summary'])\n",
        "    return df\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "def get_tfidf_vectors(texts: List[str]) -> np.ndarray:\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words=None)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# BERT Embeddings using IndoBERT\n",
        "def get_bert_embeddings(texts: List[str], model_name: str = \"indobenchmark/indobert-base-p1\") -> tuple:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        embeddings.append(embedding)\n",
        "    return np.array(embeddings), tokenizer, model\n",
        "\n",
        "# Retrieval function\n",
        "def retrieve(query: str, df: pd.DataFrame, tfidf_vectorizer, tfidf_matrix: np.ndarray,\n",
        "             bert_embeddings: np.ndarray, bert_tokenizer, bert_model, k: int = 5, use_bert: bool = True) -> List[int]:\n",
        "    query_clean = re.sub(r'[\\n\\t]', ' ', query.lower()).strip()\n",
        "    query_clean = re.sub(r'\\s+', ' ', query_clean)\n",
        "    if use_bert:\n",
        "        inputs = bert_tokenizer(query_clean, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            query_vector = bert_model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        query_vector = query_vector.reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_vector, bert_embeddings)[0]\n",
        "    else:\n",
        "        query_vector = tfidf_vectorizer.transform([query_clean]).toarray()\n",
        "        similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
        "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
        "    top_k_case_ids = df.iloc[top_k_indices]['case_id'].tolist()\n",
        "    top_k_similarities = similarities[top_k_indices]\n",
        "    return top_k_case_ids, top_k_similarities\n",
        "\n",
        "# Predict outcome function\n",
        "def predict_outcome(query: str, df: pd.DataFrame, tfidf_vectorizer, tfidf_matrix: np.ndarray,\n",
        "                    bert_embeddings: np.ndarray, bert_tokenizer, bert_model, k: int = 5) -> tuple:\n",
        "    top_k_case_ids, top_k_similarities = retrieve(query, df, tfidf_vectorizer, tfidf_matrix,\n",
        "                                                 bert_embeddings, bert_tokenizer, bert_model, k)\n",
        "    solutions = [df[df['case_id'] == cid]['solution'].iloc[0] for cid in top_k_case_ids]\n",
        "    solution_scores = {}\n",
        "    for sol, sim in zip(solutions, top_k_similarities):\n",
        "        if sol in solution_scores:\n",
        "            solution_scores[sol] += sim\n",
        "        else:\n",
        "            solution_scores[sol] = sim\n",
        "    predicted_solution = max(solution_scores, key=solution_scores.get)\n",
        "    return predicted_solution, top_k_case_ids\n",
        "\n",
        "# Evaluate retrieval performance\n",
        "def eval_retrieval(queries: List[Dict], df: pd.DataFrame, tfidf_vectorizer, tfidf_matrix: np.ndarray,\n",
        "                   bert_embeddings: np.ndarray, bert_tokenizer, bert_model, k: int = 5) -> Dict:\n",
        "    metrics = {'tfidf': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []},\n",
        "               'bert': {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}}\n",
        "    error_cases = []\n",
        "\n",
        "    for query_info in queries:\n",
        "        query = query_info['query']\n",
        "        ground_truth_id = query_info['ground_truth']\n",
        "\n",
        "        # Validate ground truth ID exists in dataset\n",
        "        if ground_truth_id not in df['case_id'].values:\n",
        "            print(f\"Warning: Ground truth ID {ground_truth_id} not found in dataset for query: {query}\")\n",
        "            continue\n",
        "\n",
        "        # TF-IDF Retrieval\n",
        "        tfidf_case_ids, tfidf_similarities = retrieve(query, df, tfidf_vectorizer, tfidf_matrix,\n",
        "                                                     bert_embeddings, bert_tokenizer, bert_model, k, use_bert=False)\n",
        "        tfidf_y_true = [1 if ground_truth_id == cid else 0 for cid in tfidf_case_ids]\n",
        "        tfidf_y_pred = [1 if cid in tfidf_case_ids else 0 for cid in [ground_truth_id] * k]\n",
        "        metrics['tfidf']['accuracy'].append(1 if ground_truth_id in tfidf_case_ids else 0)\n",
        "        metrics['tfidf']['precision'].append(precision_score([1], [1 if ground_truth_id in tfidf_case_ids else 0], zero_division=0))\n",
        "        metrics['tfidf']['recall'].append(recall_score([1], [1 if ground_truth_id in tfidf_case_ids else 0], zero_division=0))\n",
        "        metrics['tfidf']['f1'].append(f1_score([1], [1 if ground_truth_id in tfidf_case_ids else 0], zero_division=0))\n",
        "\n",
        "        # BERT Retrieval\n",
        "        bert_case_ids, bert_similarities = retrieve(query, df, tfidf_vectorizer, tfidf_matrix,\n",
        "                                                   bert_embeddings, bert_tokenizer, bert_model, k, use_bert=True)\n",
        "        bert_y_true = [1 if ground_truth_id == cid else 0 for cid in bert_case_ids]\n",
        "        bert_y_pred = [1 if cid in bert_case_ids else 0 for cid in [ground_truth_id] * k]\n",
        "        metrics['bert']['accuracy'].append(1 if ground_truth_id in bert_case_ids else 0)\n",
        "        metrics['bert']['precision'].append(precision_score([1], [1 if ground_truth_id in bert_case_ids else 0], zero_division=0))\n",
        "        metrics['bert']['recall'].append(recall_score([1], [1 if ground_truth_id in bert_case_ids else 0], zero_division=0))\n",
        "        metrics['bert']['f1'].append(f1_score([1], [1 if ground_truth_id in bert_case_ids else 0], zero_division=0))\n",
        "\n",
        "        # Error analysis with similarity scores\n",
        "        if ground_truth_id not in tfidf_case_ids or ground_truth_id not in bert_case_ids:\n",
        "            error_cases.append({\n",
        "                'query': query,\n",
        "                'ground_truth_id': ground_truth_id,\n",
        "                'tfidf_case_ids': tfidf_case_ids,\n",
        "                'tfidf_similarities': tfidf_similarities.tolist(),\n",
        "                'bert_case_ids': bert_case_ids,\n",
        "                'bert_similarities': bert_similarities.tolist()\n",
        "            })\n",
        "\n",
        "    # Average metrics\n",
        "    for model in metrics:\n",
        "        for metric in metrics[model]:\n",
        "            metrics[model][metric] = np.mean(metrics[model][metric]) if metrics[model][metric] else 0.0\n",
        "\n",
        "    return metrics, error_cases\n",
        "\n",
        "# Evaluate prediction performance\n",
        "def eval_prediction(queries: List[Dict], df: pd.DataFrame, tfidf_vectorizer, tfidf_matrix: np.ndarray,\n",
        "                    bert_embeddings: np.ndarray, bert_tokenizer, bert_model, k: int = 5) -> Dict:\n",
        "    prediction_metrics = []\n",
        "    prediction_errors = []\n",
        "\n",
        "    for query_info in queries:\n",
        "        query = query_info['query']\n",
        "        ground_truth_solution = query_info['ground_truth_solution']\n",
        "        predicted_solution, top_k_case_ids = predict_outcome(query, df, tfidf_vectorizer, tfidf_matrix,\n",
        "                                                            bert_embeddings, bert_tokenizer, bert_model, k)\n",
        "\n",
        "        # Case-insensitive exact match for prediction\n",
        "        is_correct = 1 if predicted_solution.strip().lower() == ground_truth_solution.strip().lower() else 0\n",
        "        prediction_metrics.append({\n",
        "            'query_id': query_info['query_id'],\n",
        "            'accuracy': is_correct,\n",
        "            'predicted_solution': predicted_solution,\n",
        "            'ground_truth_solution': ground_truth_solution,\n",
        "            'top_5_case_ids': top_k_case_ids\n",
        "        })\n",
        "\n",
        "        if not is_correct:\n",
        "            prediction_errors.append({\n",
        "                'query': query,\n",
        "                'predicted_solution': predicted_solution,\n",
        "                'ground_truth_solution': ground_truth_solution,\n",
        "                'top_5_case_ids': top_k_case_ids\n",
        "            })\n",
        "\n",
        "    avg_accuracy = np.mean([m['accuracy'] for m in prediction_metrics])\n",
        "    return prediction_metrics, avg_accuracy, prediction_errors\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    # Load data\n",
        "    file_path = \"/content/drive/MyDrive/Tugasbesar/data/processed/cases.csv\"\n",
        "    df = load_and_preprocess_data(file_path)\n",
        "\n",
        "    # Get TF-IDF and BERT vectors\n",
        "    tfidf_vectorizer, tfidf_matrix = get_tfidf_vectors(df['combined_text'].tolist())\n",
        "    bert_embeddings, bert_tokenizer, bert_model = get_bert_embeddings(df['combined_text'].tolist())\n",
        "\n",
        "    # Load queries\n",
        "    demo_queries = [\n",
        "        {\"query_id\": 1, \"query\": \"Sengketa perdata mengenai hutang piutang dengan penyelesaian damai\", \"ground_truth\": 1, \"ground_truth_solution\": \"Para pihak dihukum untuk mentaati akta perdamaian\"},\n",
        "        {\"query_id\": 2, \"query\": \"Wanprestasi pembayaran atas perjanjian investasi\", \"ground_truth\": 2, \"ground_truth_solution\": \"Tergugat dinyatakan wanprestasi dan wajib membayar hutang\"},\n",
        "        {\"query_id\": 3, \"query\": \"Gugatan perdata dengan bukti transfer dan pengakuan hutang\", \"ground_truth\": 3, \"ground_truth_solution\": \"Tergugat wajib membayar hutang pokok beserta bunga\"},\n",
        "        {\"query_id\": 4, \"query\": \"Pencabutan gugatan setelah mediasi tidak berhasil\", \"ground_truth\": 127, \"ground_truth_solution\": \"Perkara dicabut dari register\"},\n",
        "        {\"query_id\": 5, \"query\": \"Sengketa perdata dengan bukti setoran bank\", \"ground_truth\": 3, \"ground_truth_solution\": \"Tergugat dinyatakan wanprestasi dan wajib membayar hutang\"}\n",
        "    ]\n",
        "\n",
        "    # Evaluate retrieval\n",
        "    retrieval_metrics, error_cases = eval_retrieval(demo_queries, df, tfidf_vectorizer, tfidf_matrix,\n",
        "                                                   bert_embeddings, bert_tokenizer, bert_model, k=5)\n",
        "\n",
        "    # Evaluate prediction\n",
        "    prediction_metrics, avg_prediction_accuracy, prediction_errors = eval_prediction(demo_queries, df, tfidf_vectorizer,\n",
        "                                                                                   tfidf_matrix, bert_embeddings,\n",
        "                                                                                   bert_tokenizer, bert_model, k=5)\n",
        "\n",
        "    # Save retrieval metrics\n",
        "    retrieval_metrics_df = pd.DataFrame({\n",
        "        'Model': ['TF-IDF', 'BERT'],\n",
        "        'Accuracy': [retrieval_metrics['tfidf']['accuracy'], retrieval_metrics['bert']['accuracy']],\n",
        "        'Precision': [retrieval_metrics['tfidf']['precision'], retrieval_metrics['bert']['precision']],\n",
        "        'Recall': [retrieval_metrics['tfidf']['recall'], retrieval_metrics['bert']['recall']],\n",
        "        'F1': [retrieval_metrics['tfidf']['f1'], retrieval_metrics['bert']['f1']]\n",
        "    })\n",
        "    retrieval_metrics_path = \"/content/drive/MyDrive/Tugasbesar/data/eval/retrieval_metrics.csv\"\n",
        "    os.makedirs(os.path.dirname(retrieval_metrics_path), exist_ok=True)\n",
        "    retrieval_metrics_df.to_csv(retrieval_metrics_path, index=False)\n",
        "    print(f\"Retrieval metrics saved to {retrieval_metrics_path}\")\n",
        "    print(\"\\nRetrieval Metrics Table:\")\n",
        "    print(retrieval_metrics_df)\n",
        "\n",
        "    # Save prediction metrics\n",
        "    prediction_metrics_df = pd.DataFrame(prediction_metrics)\n",
        "    prediction_metrics_path = \"/content/drive/MyDrive/Tugasbesar/data/eval/prediction_metrics.csv\"\n",
        "    prediction_metrics_df.to_csv(prediction_metrics_path, index=False)\n",
        "    print(f\"\\nPrediction metrics saved to {prediction_metrics_path}\")\n",
        "    print(f\"Average Prediction Accuracy: {avg_prediction_accuracy:.4f}\")\n",
        "\n",
        "    # Error analysis\n",
        "    print(\"\\nError Analysis - Retrieval Failures:\")\n",
        "    for error in error_cases:\n",
        "        print(f\"Query: {error['query']}\")\n",
        "        print(f\"Ground Truth ID: {error['ground_truth_id']}\")\n",
        "        print(f\"TF-IDF Retrieved: {error['tfidf_case_ids']} (Similarities: {error['tfidf_similarities']})\")\n",
        "        print(f\"BERT Retrieved: {error['bert_case_ids']} (Similarities: {error['bert_similarities']})\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\nError Analysis - Prediction Failures:\")\n",
        "    for error in prediction_errors:\n",
        "        print(f\"Query: {error['query']}\")\n",
        "        print(f\"Predicted Solution: {error['predicted_solution']}\")\n",
        "        print(f\"Ground Truth Solution: {error['ground_truth_solution']}\")\n",
        "        print(f\"Top-5 Case IDs: {error['top_5_case_ids']}\")\n",
        "        print()\n",
        "   \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
